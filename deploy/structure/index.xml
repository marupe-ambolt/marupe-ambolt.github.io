<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Converting a notebook to API on Emily</title>
    <link>http://example.org/deploy/structure/</link>
    <description>Recent content in Converting a notebook to API on Emily</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Sep 2021 09:50:28 +0200</lastBuildDate>
    
	<atom:link href="http://example.org/deploy/structure/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>API</title>
      <link>http://example.org/deploy/structure/api/</link>
      <pubDate>Mon, 27 Sep 2021 09:50:28 +0200</pubDate>
      
      <guid>http://example.org/deploy/structure/api/</guid>
      <description>The API is how users and other applications will interact with your service. We therefore need to specify how this interaction will happen. By default, an Emily machine learning API has three endpoints specified in the api.py file related to machine learning:
 /api/train endpoint /api/evaluate endpoint /api/train endpoint  We see in the notebook that the data is taken from an online repository:
penguins = pd.read_csv(&amp;#34;https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv&amp;#34;) Since this is the only dataset that we will use, we can remove dataset_path as an input to the /api/train/ and /api/evaluate/ endpoints.</description>
    </item>
    
    <item>
      <title>Model</title>
      <link>http://example.org/deploy/structure/model/</link>
      <pubDate>Mon, 27 Sep 2021 09:50:28 +0200</pubDate>
      
      <guid>http://example.org/deploy/structure/model/</guid>
      <description>The choice of model is central to a machine learning project, since how to train, evaluate, and predict depends on the model. This choice should be implemented in the Model class, where you can customise your own model.
In the notebook we see the following snippet of code where a linear regression model is defined.
model_ols = LinearRegression() In the ml/model.py file, we therefore specify that our model should be a LinearRegression model:</description>
    </item>
    
    <item>
      <title>Training</title>
      <link>http://example.org/deploy/structure/train/</link>
      <pubDate>Mon, 27 Sep 2021 09:50:28 +0200</pubDate>
      
      <guid>http://example.org/deploy/structure/train/</guid>
      <description>The Trainer class should implement the behaviour for training a model based on the training dataset.
In the ml/trainer.py file, we can do away with any mention of the dataset_path variable, since we removed this from the /api/train/ endpoint.
_load_train_data method We first implement the _load_train_data() method, which, as the name suggests, loads the training data. To do this, we first look at the notebook and see that the data is read from a github page using the pandas library:</description>
    </item>
    
    <item>
      <title>Evaluating</title>
      <link>http://example.org/deploy/structure/eval/</link>
      <pubDate>Mon, 27 Sep 2021 09:50:28 +0200</pubDate>
      
      <guid>http://example.org/deploy/structure/eval/</guid>
      <description>The Evaluator class should implement the behaviour for evaluating a trained model based on the testing dataset.
In the ml/evaluator.py file, we can also remove mentions of the dataset_path variable. Many of the steps for evaluating are the same as for training, so we import the same libraries:
import pandas as pd from sklearn.model_selection import train_test_split from ml.model import Model _load_test_data method In order to load the test data, we do essentially the same as when loading the train data, except now we return the test data instead of the train data:</description>
    </item>
    
    <item>
      <title>Predicting</title>
      <link>http://example.org/deploy/structure/predict/</link>
      <pubDate>Mon, 27 Sep 2021 09:50:28 +0200</pubDate>
      
      <guid>http://example.org/deploy/structure/predict/</guid>
      <description>The Predictor class should implement the behaviour for predicting a new sample based on a trained model.
As a first step, we import the numpy library, which we will use later.
import numpy as np from ml.model import Model _preprocess method The _preprocess() method should take as input the features we use to predict.
 Flipper length Body mass Species  Then the method creates an array and puts the sample input into that array, and finally standardises it using the same scaler as previously, to make sure that the sample is scaled in the same way as the training data.</description>
    </item>
    
  </channel>
</rss>