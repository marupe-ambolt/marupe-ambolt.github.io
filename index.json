[
{
	"uri": "http://example.org/emily-intro/structure/api/",
	"title": "API",
	"tags": [],
	"description": "",
	"content": "The API contains endpoints that can be called by a user or another service to interact with the machine learning service implemented by the Emily project. As an example, let us have a look at the predict endpoint:\nclass PredictItem(BaseModel): sample: str model_path: str @app.post(\u0026#39;/api/predict\u0026#39;) def predict(item: PredictItem): return {\u0026#39;result\u0026#39;: emily.predict(item)} Here we can see that the predict endpoint is a POST request and is accessed through /api/predict. Furthermore, we see that it requires two parameters in the request body: a sample and a model path.\nIt is simple to change the required parameters. For example, let us say that instead of a single sample parameter, you have a model that takes date, price, and quantity as input. We can change the endpoint to handle this in the following way:\nclass PredictItem(BaseModel): date: str price: str quantity: str model_path: str @app.post(\u0026#39;/api/predict\u0026#39;) def predict(item: PredictItem): return {\u0026#39;result\u0026#39;: emily.predict(item)} "
},
{
	"uri": "http://example.org/deploy/structure/",
	"title": "Code structure",
	"tags": [],
	"description": "",
	"content": "Many AI and ML projects start as Jupyter notebooks, where one can easily explore data as well as try and compare different models. However, once the project has matured enough that a model has been selected, one should think about how to structure the code in a way that makes it easy to deploy as a service.\nWe will make use of the notebook found here. In this notebook, among other things, regression is used on a penguin dataset. We will identify the relevant parts of this regression and put them into an Emily API project. An Emily API project can be created by running emily build and choosing the Machine learning API template with the slim image (deep learning is not required).\n"
},
{
	"uri": "http://example.org/emily-intro/install/",
	"title": "Install Emily",
	"tags": [],
	"description": "",
	"content": "This guide will show you how to install Emily.\nWindows MacOS Ubuntu  First download the newest Emily release here. Run the emily-installer.exe file, which will present you with an installation wizard. Go through the steps of the installation wizard to finish the installation. During the installation, you will be asked about additional items to setup. Emily needs Docker and Visual Studio Code in order to work. You can choose to let Emily install them for you, or you can install them yourself if you prefer. The remaining tasks are optional. If you choose to install Docker, you may be prompted for admin rights, and you must accept this in order for Docker to be installed. When the installation wizard is finished, you can run Emily by opening a terminal and typing emily.  First download the newest Emily release here. Run the emily.pkg file, which will present you with an installation wizard. Follow the steps of the installation wizard to finish the installation. When the installation wizard is finished, you can run Emily by opening a terminal and typing emily.  First download the newest Emily release here. Unzip the linux.zip file, and run the emily file inside the unzipped folder. You will be asked to confirm that you want to install Emily. Press y to accept. Next you will be asked where to install Emily. Either enter the path where you would like it to be installed, or simply press enter to have it installed in the default path. Lastly you will be asked whether you want to send anonymous usage data, and then needed requirements will be installed by Emily. When this is done, you can run Emily by typing emily in the terminal.    "
},
{
	"uri": "http://example.org/emily-intro/",
	"title": "Emily intro",
	"tags": [],
	"description": "",
	"content": "Emily intro Emily is an AI platform that can help you setup, develop, and deploy AI microservices.\n"
},
{
	"uri": "http://example.org/emily-intro/build/",
	"title": "Create Emily project",
	"tags": [],
	"description": "",
	"content": "To create an Emily project, type emily build in a terminal. You will be prompted for a project name. Enter one and press enter.\n? Emily: Project name: ‣ my-emily-project Next you will be asked to choose an Emily template. These are code templates that will save you time in setting up boilerplate code for your projects. They come in different flavours, depending on what kind of project you are making. The machine learning API template should serve you well for most machine learning projects.\n? Emily: Which Emily template do you want to use? (Use arrow keys, confirm with ⎆) … Default - Minimal project with hello world script API - Simple project with API set up and ready ▸ Machine learning API - Machine learning template with API Deep neural network API - Deep neural network template with an API using Pytorch Machine learning gRPC - Machine learning template with gRPC Next you will select an Emily Docker image through a few choices. The first choice is what kind of project you will be doing. The slim project type should be enough for many machine learning projects, but if you already know that you will do a more specialised project, you could select another project type. Remember that you can always add more python packages after the project has been created.\n? Emily: Which project type best fits your case? (Use arrow keys, confirm with ⎆) … Minimal (contains just enough to run an API) ▸ Slim (contains essential Machine Learning packages) Computer Vision Natural Language Processing Next you will choose whether to use deep learning or not, and which deep learning platform you wish to use.\n? Emily: Which deep learning platform do you want to use? (Use arrow keys, confirm with ⎆) … ▸ No deep learning platform PyTorch Tensorflow Finally, choose the editor that you want to open the project in. If you change your mind later, you can always open the project in another editor by running emily open with the --editor flag.\n? Emily: Which editor do you want to use? (Use arrow keys, confirm with ⎆) … ▸ Visual Studio Code PyCharm Jupyter Notebook Jupyter Lab Emily will now build and start a Docker container and then open the project in whichever editor you chose.\n"
},
{
	"uri": "http://example.org/deploy/structure/api/",
	"title": "API",
	"tags": [],
	"description": "",
	"content": "The API is how users and other applications will interact with your service. We therefore need to specify how this interaction will happen. By default, an Emily machine learning API has three endpoints specified in the api.py file related to machine learning:\n /api/train endpoint /api/evaluate endpoint /api/train endpoint  We see in the notebook that the data is taken from an online repository:\npenguins = pd.read_csv(\u0026#34;https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv\u0026#34;) This means that we can remove dataset_path as an input to the /api/train/ and /api/evaluate/ endpoints, since we will use this fixed dataset always:\nclass TrainItem(BaseModel): save_path: str @app.post(\u0026#39;/api/train\u0026#39;) def train(item: TrainItem): return {\u0026#39;result\u0026#39;: emily.train(item)} class EvaluateItem(BaseModel): model_path: str @app.post(\u0026#39;/api/evaluate\u0026#39;) def evaluate(item: EvaluateItem): return {\u0026#39;result\u0026#39;: emily.evaluate(item)} Furthermore, if we inspect the code in the notebook, we can see that three features are used for predicting using regression:\n Flipper length Body mass Species  Because of this, we modify the /api/predict/ endpoint to take these as input:\nclass PredictItem(BaseModel): flipper_length: str body_mass: str species: str model_path: str @app.post(\u0026#39;/api/predict\u0026#39;) def predict(item: PredictItem): return {\u0026#39;result\u0026#39;: emily.predict(item)} Full file The api.py file now looks like this:\nimport uvicorn import os from fastapi import FastAPI from fastapi.middleware.cors import CORSMiddleware from pydantic import BaseModel from dotenv import load_dotenv from argparse import ArgumentParser from utilities.utilities import get_uptime from utilities.logging.config import initialize_logging, initialize_logging_middleware from ml.emily import Emily emily = Emily() # --- Welcome to your Emily API! --- # # See the README for guides on how to test it. # Your API endpoints under http://yourdomain/api/... # are accessible from any origin by default. # Make sure to restrict access below to origins you # trust before deploying your API to production. parser = ArgumentParser() parser.add_argument(\u0026#39;-e\u0026#39;, \u0026#39;--env\u0026#39;, default=\u0026#39;.env\u0026#39;, help=\u0026#39;sets the environment file\u0026#39;) args = parser.parse_args() dotenv_file = args.env load_dotenv(dotenv_file) app = FastAPI() initialize_logging() initialize_logging_middleware(app) app.add_middleware( CORSMiddleware, allow_origins=[\u0026#34;*\u0026#34;], allow_credentials=True, allow_methods=[\u0026#34;*\u0026#34;], allow_headers=[\u0026#34;*\u0026#34;], ) @app.get(\u0026#39;/api/health\u0026#39;) def health_check(): return { \u0026#39;uptime\u0026#39;: get_uptime(), \u0026#39;status\u0026#39;: \u0026#39;UP\u0026#39;, \u0026#39;port\u0026#39;: os.environ.get(\u0026#34;HOST_PORT\u0026#34;), } @app.get(\u0026#39;/api\u0026#39;) def hello(): return f\u0026#39;The API is running (uptime: {get_uptime()})\u0026#39; class TrainItem(BaseModel): save_path: str @app.post(\u0026#39;/api/train\u0026#39;) def train(item: TrainItem): return {\u0026#39;result\u0026#39;: emily.train(item)} class EvaluateItem(BaseModel): model_path: str @app.post(\u0026#39;/api/evaluate\u0026#39;) def evaluate(item: EvaluateItem): return {\u0026#39;result\u0026#39;: emily.evaluate(item)} class PredictItem(BaseModel): flipper_length: str body_mass: str species: str model_path: str @app.post(\u0026#39;/api/predict\u0026#39;) def predict(item: PredictItem): return {\u0026#39;result\u0026#39;: emily.predict(item)} if __name__ == \u0026#39;__main__\u0026#39;: uvicorn.run( \u0026#39;api:app\u0026#39;, host=os.environ.get(\u0026#39;HOST_IP\u0026#39;), port=int(os.environ.get(\u0026#39;CONTAINER_PORT\u0026#39;)) ) "
},
{
	"uri": "http://example.org/deploy/test/",
	"title": "API test",
	"tags": [],
	"description": "",
	"content": "Once you have set up your API, it is time to test that it actually works. There are multiple ways to do this, and you will see here three different tools for testing your API:\n Swagger UI Postman curl  First of all, you should run your api. In an emily project, you can do this by running the API script with python api.py inside the container. We will assume here that you are running your API on http://localhost:4242. Adjust accordingly if you are using a different host or port.\nNote that the evaluate and predict endpoints in an Emily project usually requires a model to already have been trained through the train endpoint.\nSwagger UI Postman curl  Emily projects use FastAPI, which automatically supports Swagger UI. This gives you automatic documentation of your API, as well as a graphical interface for testing the API. To access Swagger UI, go to http://localhost:4242/docs in your browser, where you will see something similar to the below image. To test e.g. the evaluate endpoint, click its bar. It will then unfold, giving you a lot of information about the endpoint, such as the format of a request to the endpoint and the possible responses it can give. In order to test it, click the \"Try it out\" button. You will then be able to edit the request body before sending the request. Make appropriate changes, such as changing the path to the model being evaluated, and then click the \"Execute\" button below the text field. You will then get the result of the endpoint in a section below. In this case, the result is an evaluation score for the specified model.  Postman is a standalone app that can be downloaded here. Start Postman and create a new request. Choose a POST request, and enter http://localhost:4242 as the request URL. Next we need to specify the body of the request. To do this, press the \"Body\" button below the URL field and choose the \"raw\" option below it. Now you can specify the request body in JSON format in the text field below. For example, to specify the model path, enter { \"model_path\": \"myModel.save\" } Now you can simply press the \"Send\" button, and the request will be sent to the API and a response will be shown at the bottom of the window. In this case the response is an evaluation score for the specified model.  curl is a command-line tool that can be downloaded here. Once you have installed curl, you can invoke it from the terminal by simply typing curl. The template for making an API request with curl is curl --header \"Content-Type: application/json\" --request POST --data '\u0026lt;JSON data\u0026gt;' \u0026lt;URL\u0026gt; For example, to call the evaluate endpoint with a specific model, we can use curl --header \"Content-Type: application/json\" --request POST --data '{\"model_path\":\"myModel.save\"}' http://0.0.0.0:4242/api/evaluate    "
},
{
	"uri": "http://example.org/emily-intro/structure/emily/",
	"title": "Emily",
	"tags": [],
	"description": "",
	"content": "The Emily class is simply a wrapper class that instantiates a trainer, an evaluator, and a predictor, and makes these available for calling through the API.\ndef __init__(self): self.predictor = Predictor() # Creates instance of the Predictor class self.trainer = Trainer() # Creates instance of the Trainer class self.evaluator = Evaluator() # Creates instance of the Evaluator class def predict(self, request): \u0026#34;\u0026#34;\u0026#34; This function calls the __call__ function from the Predictor class in ml.predictor.py. Make sure the __call__ method is implemented correctly \u0026#34;\u0026#34;\u0026#34; return self.predictor(request) def train(self, request): \u0026#34;\u0026#34;\u0026#34; This function calls the __call__ function from the Trainer class in ml.trainer.py. Make sure the __call__ method is implemented correctly \u0026#34;\u0026#34;\u0026#34; return self.trainer(request) def evaluate(self, request): \u0026#34;\u0026#34;\u0026#34; This function calls the __call__ function from the Evaluator class in ml.evaluator.py. Make sure the __call__ method is implemented correctly \u0026#34;\u0026#34;\u0026#34; return self.evaluator(request) "
},
{
	"uri": "http://example.org/deploy/",
	"title": "Deploy to production",
	"tags": [],
	"description": "",
	"content": "Deploy to production AI services running on your own computer can be fun, but in order to let other people use them and actually make an impact, you must put them into production.\n"
},
{
	"uri": "http://example.org/deploy/deploy/",
	"title": "Deploy",
	"tags": [],
	"description": "",
	"content": "Getting your service into production involves deploying it to a server or similar where it is hosted and can be accessed by clients, apps, or other services. Once you have a server setup that you want to deploy to, Emily can help you with the actual deployment.\nTo deploy, run emily deploy server \u0026lt;project name\u0026gt;. First you will be asked which file to run when deployed. For Emily projects this is usually api.py. Next you must choose whether to use GPU functionality on the server you deploy on. If you wish to use a specific port or host on the server, you can specify these with the --host and --port flags. For example, emily deploy server my-emily-project --host 0.0.0.0 --port 4242 will deploy to host 0.0.0.0 and port 4242. After you have run the deploy command, a number of files will have been generated, most importantly the deploy script deploy.sh.\nNext you must transfer the project directory to the server. This can be done either by using a version control system like git, or by using a direct transfer protocol like scp. Note that if you have sensitive information in your project, such as user data, you may want to ensure that this part of the project is not transferred to the server.\nOnce the project files are on the server, run sh deploy.sh inside the project folder on the server. This will ensure that Docker is installed on the server, and then start a Docker container running the service. Your service is now deployed and ready to be used!\n"
},
{
	"uri": "http://example.org/deploy/structure/model/",
	"title": "Model",
	"tags": [],
	"description": "",
	"content": "In the notebook we see the following snippet of code where a linear regression model is defined.\nmodel_ols = LinearRegression() In the model.py file, we therefore specify that our model should be a LinearRegression model:\nfrom sklearn.linear_model import LinearRegression class Model(LinearRegression): We also add a scaler to the model, which we will use to standardise data at various stages:\nfrom sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression def __init__(self): super().__init__() # Inherit methods from the super class which this class extends from self.scaler = StandardScaler() forward method When forwarding a sample to the model, it should simply predict using the predict() method:\ndef forward(self, sample): return self.predict(sample) save_model and load_model methods Finally, to save and load a model, we use the pickle library, which must be imported:\nimport pickle from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression Now we can implement saving and loading:\ndef save_model(self, save_path): with open(save_path, \u0026#39;wb\u0026#39;) as fp: pickle.dump(self, fp) def load_model(self, model_path): with open(model_path, \u0026#39;rb\u0026#39;) as fp: model = pickle.load(fp) self.__dict__.update(model.__dict__) Full file The model.py file now looks like this:\nimport pickle from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression class Model(LinearRegression): def __init__(self): super().__init__() # Inherit methods from the super class which this class extends from self.scaler = StandardScaler() def forward(self, sample): return self.predict(sample) def save_model(self, save_path): with open(save_path, \u0026#39;wb\u0026#39;) as fp: pickle.dump(self, fp) def load_model(self, model_path): with open(model_path, \u0026#39;rb\u0026#39;) as fp: model = pickle.load(fp) self.__dict__.update(model.__dict__) def __call__(self, sample): return self.forward(sample) "
},
{
	"uri": "http://example.org/emily-intro/structure/",
	"title": "Structure",
	"tags": [],
	"description": "",
	"content": "Here you will see an overview of the structure of an Emily machine learning project with an API. Other Emily templates may omit some of these parts, and other templates may contain additional parts, but most templates use this structure as a baseline.\nAt its core, an Emily machine learning project consists of six parts:\n API: The API is where users or other services will access the service. It has endpoints for training, evaluating, and predicting using a machine learning model. The API calls the Trainer, Evaluator, and Predictor through the Emily class. Emily: The Emily class is a wrapper class whose only purpose is to aggregate the Trainer, Evaluator, and Predictor. Trainer: The Trainer class specifies how a model is trained when the train endpoint is called in the API. The Trainer makes use of the Model class to instantiate a machine learning model and to save it when the training is complete. Evaluator: The Evaluator class specifies how a model is evaluated when the evaluate endpoint is called in the API. The Evaluator makes use of the Model class to load a machine learning model that has already been trained by the Trainer. Predictor: The Predictor class specifies how to use a model to make predictions when the predict endpoint is called in the API. The Predictor makes use of the Model class to load a machine learning model that has already been trained by the Trainer. Model: The Model class specifies what kind of machine learning model should be used in the project, and also how this kind of model can be saved and loaded from memory.  "
},
{
	"uri": "http://example.org/emily-intro/structure/trainer/",
	"title": "Trainer",
	"tags": [],
	"description": "",
	"content": "The trainer implements the behaviour for training a model. The steps for doing so are the following:\n Load training data Preprocess training data Train using the training data Save the trained model  1. Load training data In order to load the training data, the _load_train_data method must be implemented. For example, the training data can be loaded by reading a csv file using pandas:\ndef _load_train_data(self, dataset_path): train_dataset = pandas.read_csv(dataset_path) return train_dataset 2. Preprocess training data Preprocessing of the training data must be implemented in the _preprocess_train_data method. For example, this could involve dropping those parts of the data with missing data:\ndef _preprocess_train_data(self, train_data): preprocessed_train_data = train_data.dropna() return preprocessed_train_data 3. Train using the training data The actual training will be done in the train method after calls to the _load_train_data and _preprocess_train_data methods. For example, if using a scikit-learn model, training be done this way:\nself.model.fit(X, y) 4. Save the trained model Finally, the trained model should be saved, so it can be used later by the evaluate and predict endpoints. This is done by simply using the save_model method on the model class:\nreturn self.model.save_model(save_path) "
},
{
	"uri": "http://example.org/emily-intro/structure/evaluator/",
	"title": "Evaluator",
	"tags": [],
	"description": "",
	"content": "The evaluator implements the behaviour for evaluating a model. The steps for doing so are the following:\n Load the model Load the test data Preprocess the test data Evaluate the model on the test data  1. Load the model In order to load the model, use the load_model method on the model class. Here is how this is done by default in the template:\nif model_path != self.model_path: self.model = Model() self.model.load_model(model_path) self.model_path = model_path This ensures that the same model is not loaded twice, which may be time-consuming for large models. However, if you are using small models and you want to be able to reuse the same model when retraining, the if-check may not be necessary.\n2. Load the test data Loading the test data must be implemented in the _load_test_data method. For example, if you are reading the test data from a csv file, it can be done with pandas in the following way:\ndef _load_test_data(self, dataset_path): test_dataset = pandas.read_csv(dataset_path) return test_dataset 3. Preproces the test data Preprocessing must be implemented in the _preprocess_test_data method. For example, this could involve dropping those parts of the data with missing data:\ndef _preprocess_test_data(self, test_data): preprocessed_test_data = test_data.dropna() return preprocessed_test_data 4. Evaluate the model on the test data Finally, the actual evaluation of the model will be done in the evaluate method. For example, if using an scikit-learn model, evaluating can be done this way:\nscore = self.model.score(X, y) "
},
{
	"uri": "http://example.org/emily-intro/share/",
	"title": "Share Emily projects",
	"tags": [],
	"description": "",
	"content": "When working in a team, it is important that machine learning projects can be shared among team members. Emily supports easy sharing of projects, and integrates with version control software such as git.\nIn order to open an Emily project created on another computer, simply copy the project folder onto your own computer, either by standard file copying or through a version control software such as git. Let\u0026rsquo;s say the folder you have copied is called my-emily-project. Then simply navigate to where the folder is located and type emily open my-emily-project.\nYou will then be asked which editor you want to use.\n? Emily: Which editor do you want to use? (Use arrow keys, confirm with ⎆) … ▸ Visual Studio Code PyCharm Jupyter Notebook Jupyter Lab Choose whichever editor you would like to use, and Emily will then import the project into your local configuration, start the Docker container, and open your project in the chosen editor.\n"
},
{
	"uri": "http://example.org/deploy/structure/train/",
	"title": "Training",
	"tags": [],
	"description": "",
	"content": "In the trainer.py file, we can do away with any mention of the dataset_path variable, since we removed this from the /api/train/ endpoint.\n_load_train_data method We first implement the _load_train_data() method, which, as the name suggests, loads the training data. To do this, we first look at the notebook and see that the data is read from a github page using the pandas library:\npenguins = pd.read_csv(\u0026#34;https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv\u0026#34;) Therefore we need to import the pandas library, and we will also import the train_test_split library from scikitlearn:\nimport pandas as pd from sklearn.model_selection import train_test_split from ml.model import Model Now we can use pandas to read the data:\ndef _load_train_data(self): data = pd.read_csv(\u0026#34;https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv\u0026#34;) return data _preprocess_train_data method We see in the following lines of code from the notebook how to split the data into feature variables (X) and target variable (y):\npenguins.dropna(inplace=True) y = penguins[\u0026#39;culmen_length_mm\u0026#39;] X_dum = penguins.species_short.str.get_dummies() X = pd.concat([penguins.iloc[:,4:6], X_dum], axis=1) X.iloc[:,0:2] = StandardScaler().fit_transform(X.iloc[:,0:2]) from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 21) We put these parts of the notebook into the _preprocess_train_data() method:\ndef _preprocess_train_data(self, train_data): train_data.dropna(inplace=True) y = train_data[\u0026#39;culmen_length_mm\u0026#39;] X_dum = train_data.species_short.str.get_dummies() X = pd.concat([train_data.iloc[:,4:6], X_dum], axis=1) X.iloc[:,0:2] = self.model.scaler.fit_transform(X.iloc[:,0:2]) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21) return X_train, y_train train method First we modify the existing code so it conforms to the modifications we have made in the previous methods:\ndef train(self, request): \u0026#34;\u0026#34;\u0026#34; Starts the training of a model based on data loaded by the self._load_train_data function \u0026#34;\u0026#34;\u0026#34; # Unpack request save_path = request.save_path # Read the dataset from the dataset_path data = self._load_train_data() # Preprocess the dataset X_train, y_train = self._preprocess_train_data(data) The relevant code for actually training the model in the notebook is:\nmodel_ols.fit(X_train, y_train) We duplicate this in the train() method:\nself.model.fit(X_train, y_train) # Save the trained model return self.model.save_model(save_path) Full file The trainer.py file now looks like this:\nimport pandas as pd from sklearn.model_selection import train_test_split from ml.model import Model class Trainer: \u0026#34;\u0026#34;\u0026#34; The Trainer class is used for training a model instance based on the Model class found in ml.model.py. In order to get started with training a model the following steps needs to be taken: 1. Define the Model class in ml.model.py 2. Prepare train data on which the model should be trained with by implementing the _read_train_data() function and the _preprocess_train_data() function \u0026#34;\u0026#34;\u0026#34; def __init__(self): self.model = Model() # creates an instance of the Model class (see guidelines in ml.model.py) def train(self, request): \u0026#34;\u0026#34;\u0026#34; Starts the training of a model based on data loaded by the self._load_train_data function \u0026#34;\u0026#34;\u0026#34; # Unpack request save_path = request.save_path # Read the dataset from the dataset_path data = self._load_train_data() # Preprocess the dataset X_train, y_train = self._preprocess_train_data(data) self.model.fit(X_train, y_train) # Save the trained model return self.model.save_model(save_path) def _load_train_data(self): data = pd.read_csv(\u0026#34;https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv\u0026#34;) return data def _preprocess_train_data(self, data): data.dropna(inplace=True) y = data[\u0026#39;culmen_length_mm\u0026#39;] X_dum = data.species_short.str.get_dummies() X = pd.concat([data.iloc[:,4:6], X_dum], axis=1) X.iloc[:,0:2] = self.model.scaler.fit_transform(X.iloc[:,0:2]) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21) return X_train, y_train def __call__(self, request): return self.train(request) "
},
{
	"uri": "http://example.org/deploy/structure/eval/",
	"title": "Evaluating",
	"tags": [],
	"description": "",
	"content": "In the evaluator.py file, we can also remove mentions of the dataset_path variable. Many of the steps for evaluating are the same as for training, so we import the same libraries:\nimport pandas as pd from sklearn.model_selection import train_test_split from ml.model import Model _load_test_data method In order to load the test data, we do essentially the same as when loading the train data, except now we return the test data instead of the train data.\ndef _load_test_data(self): data = pd.read_csv(\u0026#34;https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv\u0026#34;) return data _preprocess_test_data method Likewise for the preprocessing step, we mirror the steps taken for the train data\ndef _preprocess_test_data(self, test_data): test_data.dropna(inplace=True) y = test_data[\u0026#39;culmen_length_mm\u0026#39;] X_dum = test_data.species_short.str.get_dummies() X = pd.concat([test_data.iloc[:,4:6], X_dum], axis=1) X.iloc[:,0:2] = self.model.scaler.transform(X.iloc[:,0:2]) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21) return X_test, y_test Note the random_state=21 parameter, which ensures that the data is split the same way each time.\nevaluate method First we modify the existing code to match the changes made to the _load_test_data() and _preprocess_test_data methods:\ndef evaluate(self, request): \u0026#34;\u0026#34;\u0026#34; Evaluates a trained model located at \u0026#39;model_path\u0026#39; based on test data from the self._load_test_data function \u0026#34;\u0026#34;\u0026#34; # Unpack request model_path = request.model_path # Loads a trained instance of the Model class # If no model has been trained yet proceed to follow the steps in ml.trainer.py if model_path != self.model_path: self.model = Model() self.model.load_model(model_path) self.model_path = model_path # Read the dataset from the dataset_path data = self._load_test_data() # Preprocess dataset to prepare it for the evaluator X_test, y_test = self._preprocess_test_data(data) We see in the notebook that the model is evaluated like this:\nmodel_ols.score(X_test, y_test) Putting this into the evaluate() method we obtain:\nscore = self.model.score(X_test, y_test) return score Full file The evaluator.py file now looks like this:\nimport pandas as pd from sklearn.model_selection import train_test_split from ml.model import Model class Evaluator: \u0026#34;\u0026#34;\u0026#34; The Evaluator class is used for evaluating a trained model instance. In order to get started with evaluating a model the following steps needs to be taken: 1. Train a model following the steps in ml.trainer.py 2. Prepare test data on which the model should be evaluated on by implementing the _read_test_data() function and the _preprocess_test_data function \u0026#34;\u0026#34;\u0026#34; def __init__(self): self.model_path = \u0026#34;\u0026#34; self.model = None def evaluate(self, request): \u0026#34;\u0026#34;\u0026#34; Evaluates a trained model located at \u0026#39;model_path\u0026#39; based on test data from the self._load_test_data function \u0026#34;\u0026#34;\u0026#34; # Unpack request model_path = request.model_path # Loads a trained instance of the Model class # If no model has been trained yet proceed to follow the steps in ml.trainer.py if model_path != self.model_path: self.model = Model() self.model.load_model(model_path) self.model_path = model_path # Read the dataset from the dataset_path data = self._load_test_data() # Preprocess dataset to prepare it for the evaluator X_test, y_test = self._preprocess_test_data(data) score = self.model.score(X_test, y_test) return score def _load_test_data(self): data = pd.read_csv(\u0026#34;https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv\u0026#34;) return data def _preprocess_test_data(self, data): data.dropna(inplace=True) y = data[\u0026#39;culmen_length_mm\u0026#39;] X_dum = data.species_short.str.get_dummies() X = pd.concat([data.iloc[:,4:6], X_dum], axis=1) X.iloc[:,0:2] = self.model.scaler.transform(X.iloc[:,0:2]) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21) return X_test, y_test def __call__(self, request): return self.evaluate(request) "
},
{
	"uri": "http://example.org/emily-intro/structure/predictor/",
	"title": "Predictor",
	"tags": [],
	"description": "",
	"content": "The predictor implements the behaviour for using a model to make a prediction based on a sample. The steps for doing so are the following:\n Load the model Preprocess the sample Make the prediction Postprocess the prediction  1. Load the model In order to load the model, use the load_model method on the model class. Here is how this is done by default in the template:\nif model_path != self.model_path: self.model = Model() self.model.load_model(model_path) self.model_path = model_path 2. Preprocess the sample Preprocessing must be implemented in the _preprocess method. For example, this could involve doing a label encoding for string values:\ndef _preprocess(self, sample): if sample == \u0026#34;low\u0026#34;: return 0 if sample == \u0026#34;medium\u0026#34;: return 1 if sample == \u0026#34;high\u0026#34;: return 2 3. Make the prediction Making a prediction is done in the predict method by simply forwarding the sample to the model:\nprediction = self.model(preprocessed_sample) 4. Postprocess the prediction Finally, it may be beneficial or necessary to make some adjustments to the prediction before sending it as a response to the API request. For example, this could involve unpacking the result from a numpy array and adding a dollar sign to show the unit and currency of the result:\ndef _postprocess(self, prediction): return \u0026#34;$\u0026#34; + str(prediction[0][0]) "
},
{
	"uri": "http://example.org/emily-intro/structure/model/",
	"title": "Model",
	"tags": [],
	"description": "",
	"content": "The model is where the type of machine learning model to be used in the Emily project is specified. This should be done by letting the model class inherit from whichever machine learning model you wish to use in the project. For example, if you want to use a linear regression model, you can use the LinearRegression class from scikit-learn:\nclass Model(sklearn.linear_model.LinearRegression): When we forward a sample to the model, it should make a prediction. This should be implemented in the forward method. For example, if you are using a scikit-learn model, this can be done in the following way:\ndef forward(self, sample): return self.predict(sample) Finally, we must implement saving and loading a model in the save_model and load_model methods. Both can for example be done using the pickle library. Saving a model using pickle can be done in the following way:\ndef save_model(self, save_path): with open(save_path, \u0026#39;wb\u0026#39;) as fp: pickle.dump(self, fp) Likewise, loading a model using pickle can be done like this:\ndef load_model(self, model_path): with open(model_path, \u0026#39;rb\u0026#39;) as fp: model = pickle.load(fp) self.__dict__.update(model.__dict__) "
},
{
	"uri": "http://example.org/deploy/structure/predict/",
	"title": "Predicting",
	"tags": [],
	"description": "",
	"content": "Now that everything else has been set up, we are ready for doing predictions. As a first step, we import the numpy library, which we will use later.\nimport numpy as np from ml.model import Model _preprocess method The _preprocess() method should take as input the features we use to predict.\n Flipper length Body mass Species  Then the method creates an array and puts the sample input into that array, and finally standardises it using the same scaler as previously, to make sure that the sample is scaled in the same way as the training data.\ndef _preprocess(self, flipper_length, body_mass, species): array = np.zeros((1,5), dtype = int) array[0][0] = flipper_length array[0][1] = body_mass array[0][2] = 0 array[0][3] = 0 array[0][4] = 0 if (species == \u0026#39;Adelie\u0026#39;): array[0][2] = 1 if (species == \u0026#39;Chinstrap\u0026#39;): array[0][3] = 1 if (species == \u0026#39;Gentoo\u0026#39;): array[0][4] array[0,0:2] = self.model.scaler.transform(array[:,0:2]) return array _postprocess method For the postprocessing step, we simply add a bit of explanatory text to the result:\ndef _postprocess(self, prediction): return \u0026#34;Predicted culmen length is \u0026#34; + str(prediction[0]) + \u0026#34; mm\u0026#34; train method For the actual training, all we need to do is to make sure to unpack each of the features in the sample, as well as modify the code slightly to fit the changes made to the other methods:\ndef predict(self, request): \u0026#34;\u0026#34;\u0026#34; Performs prediction on a sample using the model at the given path \u0026#34;\u0026#34;\u0026#34; # Unpack request flipper_length = request.flipper_length body_mass = request.body_mass species = request.species model_path = request.model_path # Loads a trained instance of the Model class # If no model has been trained yet proceed to follow the steps in ml.trainer.py if model_path != self.model_path: self.model = Model() self.model.load_model(model_path) self.model_path = model_path # Preprocess the inputted sample to prepare it for the model preprocessed_sample = self._preprocess(flipper_length, body_mass, species) # Forward the preprocessed sample into the model as defined in the __call__ function in the Model class prediction = self.model(preprocessed_sample) # Postprocess the prediction to prepare it for the client prediction = self._postprocess(prediction) return prediction Full file The predictory.py file now looks like this:\nimport numpy as np from ml.model import Model class Predictor: \u0026#34;\u0026#34;\u0026#34; The Predictor class is used for making predictions using a trained model instance based on the Model class defined in ml.model.py and the training steps defined in ml.trainer.py \u0026#34;\u0026#34;\u0026#34; def __init__(self): self.model_path = \u0026#34;\u0026#34; self.model = None def predict(self, request): \u0026#34;\u0026#34;\u0026#34; Performs prediction on a sample using the model at the given path \u0026#34;\u0026#34;\u0026#34; # Unpack request flipper_length = request.flipper_length body_mass = request.body_mass species = request.species model_path = request.model_path # Loads a trained instance of the Model class # If no model has been trained yet proceed to follow the steps in ml.trainer.py if model_path != self.model_path: self.model = Model() self.model.load_model(model_path) self.model_path = model_path # Preprocess the inputted sample to prepare it for the model preprocessed_sample = self._preprocess(flipper_length, body_mass, species) # Forward the preprocessed sample into the model as defined in the __call__ function in the Model class prediction = self.model(preprocessed_sample) # Postprocess the prediction to prepare it for the client prediction = self._postprocess(prediction) return prediction def _preprocess(self, flipper_length, body_mass, species): array = np.zeros((1,5), dtype = int) array[0][0] = flipper_length array[0][1] = body_mass array[0][2] = 0 array[0][3] = 0 array[0][4] = 0 if (species == \u0026#39;Adelie\u0026#39;): array[0][2] = 1 if (species == \u0026#39;Chinstrap\u0026#39;): array[0][3] = 1 if (species == \u0026#39;Gentoo\u0026#39;): array[0][4] array[0,0:2] = self.model.scaler.transform(array[:,0:2]) return array def _postprocess(self, prediction): return \u0026#34;Predicted culmen length is \u0026#34; + str(prediction[0]) + \u0026#34; mm\u0026#34; def __call__(self, request): return self.predict(request) "
},
{
	"uri": "http://example.org/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Emily Welcome to this Emily website\n"
},
{
	"uri": "http://example.org/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]